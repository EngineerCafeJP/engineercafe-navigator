# AI エージェント開発のためのアジャイルガイド

このドキュメントでは、LangGraph エージェントをアジャイルに開発するためのベストプラクティスを説明します。

## スプリント計画

### 推奨スプリント構成

**スプリント期間**: 2週間（AI開発のイテレーションサイクルに最適）

```
Week 1:
├── Day 1-2: 計画・設計・プロンプトエンジニアリング
├── Day 3-5: 実装・単体テスト
└── Day 5: 中間チェックポイント

Week 2:
├── Day 1-3: 統合テスト・評価
├── Day 4: ドキュメント・改善
└── Day 5: スプリントレビュー・レトロスペクティブ
```

### タスク分解例

**Epic**: BusinessInfoAgent の Enhanced RAG 統合

```
User Stories:
├── ユーザーとして、営業時間を正確に知りたい
├── 開発者として、RAG 検索をデバッグしたい
└── システムとして、エンティティ優先度を適用したい

Tasks:
├── プロンプトエンジニアリング (1-2日)
│   ├── システムプロンプト設計
│   ├── Few-shot 例作成
│   └── プロンプトバリエーションテスト
├── RAG 統合 (2-3日)
│   ├── ベクトル検索実装
│   ├── エンティティ優先度スコアリング
│   └── 実用アドバイス生成
├── テスト・評価 (1-2日)
│   ├── 単体テスト作成
│   ├── 評価データセット作成 (20件以上)
│   └── LLM-as-Judge 評価
└── ドキュメント (0.5-1日)
```

## Definition of Done (完了の定義)

### エージェント実装の完了条件

**機能完成度**
- [ ] 主要ユースケースで 90%+ の精度
- [ ] エッジケースの適切なハンドリング
- [ ] RouterAgent との正常連携
- [ ] 統一レスポンス形式 (UnifiedAgentResponse)

**プロンプトエンジニアリング**
- [ ] プロンプトバージョン管理 (例: v1.2.0)
- [ ] ペルソナと制約の明確化
- [ ] Few-shot 例でエッジケースカバー
- [ ] パフォーマンスベースライン文書化

**テスト・評価**
- [ ] 単体テストカバレッジ 80%+ (非LLMロジック)
- [ ] 評価データセット 20件以上
- [ ] LLM-as-Judge 評価 85%+ パス
- [ ] 人間評価 10サンプル以上
- [ ] パフォーマンスベンチマーク SLA 達成

**オブザーバビリティ**
- [ ] 構造化ログ出力
- [ ] OpenTelemetry トレース
- [ ] パフォーマンスメトリクス
- [ ] 異常検知アラート設定

**ドキュメント**
- [ ] エージェント README 更新
- [ ] API ドキュメント更新
- [ ] プロンプト変更履歴
- [ ] 既知の制限事項

## コードレビューガイドライン

### AI 固有のレビューチェックリスト

**プロンプトエンジニアリング**
```
- [ ] ペルソナが明確に定義されているか
- [ ] 指示が具体的で曖昧さがないか
- [ ] 有害な出力を防ぐ制約があるか
- [ ] Few-shot 例が多様でバランスが取れているか
```

**LLM 出力ハンドリング**
```
- [ ] 非決定的な出力が適切に処理されているか
- [ ] リトライロジックが実装されているか
- [ ] ハルシネーション検出・フラグ付けがあるか
- [ ] 出力バリデーションがあるか
```

**RAG システム**
```
- [ ] 埋め込みモデルが一貫しているか (text-embedding-3-small)
- [ ] 検索結果の重複排除があるか
- [ ] フォールバック戦略が定義されているか
```

### レビュー SLA

| PR タイプ | 目標時間 |
|----------|---------|
| プロンプト調整・設定変更 | 4時間以内 |
| 新エージェント機能 | 24時間以内 |
| アーキテクチャ変更 | 48時間以内 |

## テスト戦略

### テストピラミッド

```
           ┌─────────┐
           │  Human  │  (週次: 20-50サンプル)
           │  Eval   │
          ┌┴─────────┴┐
          │ LLM-Judge │  (PR時: 85%+ パス率)
          │   Eval    │
         ┌┴───────────┴┐
         │ Integration  │  (コミット時: 2分タイムアウト)
         │    Tests     │
        ┌┴─────────────┴┐
        │   Unit Tests   │  (コミット時: 80%+ カバレッジ)
        └────────────────┘
```

### 単体テスト例

```python
@pytest.mark.asyncio
async def test_business_info_agent_hours_query():
    """営業時間クエリのテスト"""
    state = {
        "query": "エンジニアカフェの営業時間は？",
        "language": "ja",
        "session_id": "test-123"
    }

    result = await business_info_node(state)

    assert result["success"] == True
    assert "9:00" in result["response"]
    assert "22:00" in result["response"]
```

### LLM-as-Judge 評価例

```python
evaluator = LLMJudge({
    "model": "gpt-4o",
    "criteria": {
        "accuracy": "回答は質問に正確に答えているか？",
        "relevance": "回答は質問に焦点を当てているか？",
        "completeness": "十分な情報を提供しているか？",
        "tone": "フレンドリーで親切なトーンか？"
    }
})

# ターゲット: 85%+ パス率
```

## プロンプトバージョン管理

### セマンティックバージョニング

```
MAJOR.MINOR.PATCH

MAJOR: 破壊的変更（レスポンス構造変更など）
MINOR: 新機能追加（後方互換性あり）
PATCH: バグ修正・軽微な改善
```

### バージョン例

```
v1.0.0 → v1.0.1  # 誤字修正
v1.0.1 → v1.1.0  # 「土曜日」コンテキストクエリ対応追加
v1.1.0 → v2.0.0  # シングルターン → マルチターン対応
```

### プロンプト変更履歴

```markdown
## RouterAgent

### v1.3.0 (2025-07-02)
**タイプ**: Minor
**変更内容**:
- 「も」助詞のコンテキスト依存クエリ対応
- メモリ統合の強化
- 地下施設キーワード検出の改善

**パフォーマンス影響**:
- ルーティング精度: 88.2% → 94.1% (+5.9%)
```

## モニタリングとオブザーバビリティ

### 追跡すべきメトリクス

**システムパフォーマンス**
- レスポンスタイム (P50, P95, P99)
- 成功率 / エラー率
- スループット (RPS)

**AI 固有メトリクス**
- トークン使用量 / コスト
- ルーティング精度
- RAG 検索精度
- キャッシュヒット率

### アラート閾値

| メトリクス | Warning | Critical |
|-----------|---------|----------|
| エラー率 | > 3% | > 5% |
| P95 レイテンシ | > baseline×1.5 | > baseline×2 |
| ルーティング精度 | < 90% | < 85% |
| 日次トークン使用量 | > budget×80% | > budget×95% |

## GitHub ラベル

### 優先度ラベル

```
🔴 P0-Critical    # 本番ダウン、セキュリティ脆弱性
🟠 P1-High        # ブロッキング機能、重大バグ
🟡 P2-Medium      # 重要だがブロッキングではない
🟢 P3-Low         # あると良い、軽微な改善
```

### タイプラベル

```
✨ type:feature       # 新エージェント・機能
🐛 type:bug           # バグ修正
📚 type:docs          # ドキュメント更新
🎨 type:prompt        # プロンプトエンジニアリング
🧪 type:evaluation    # 評価・テスト
```

### エージェントラベル

```
🤖 agent:router           # RouterAgent
💼 agent:business-info    # BusinessInfoAgent
🏢 agent:facility         # FacilityAgent
🧠 agent:memory           # MemoryAgent
📅 agent:event            # EventAgent
```

## チーム儀式

### デイリースタンドアップ (15分)

```
1. 昨日取り組んだエージェント/機能は？
2. 今日の作業内容は？
3. ブロッカーはあるか？

AI 固有の質問:
- 予期しない LLM 動作はあったか？
- プロンプトバージョン変更はあったか？
- 評価メトリクスの傾向は？
```

### スプリントレビュー (1時間)

```
デモ:
- ライブエージェントデモンストレーション
- 評価メトリクス (before/after)
- ルーティング精度の改善
- 新機能のデモ
```

### レトロスペクティブ (1時間)

```
振り返り質問:
- 効果的だったプロンプトエンジニアリング技法は？
- 最も価値のあった評価手法は？
- エージェント品質をより速く向上させるには？
- ツールのギャップは何か？
```

## 参考リソース

- [LangGraph ドキュメント](https://langchain-ai.github.io/langgraph/)
- [OpenTelemetry](https://opentelemetry.io/)
- [Promptfoo](https://promptfoo.dev/) - プロンプト評価ツール
- [LangSmith](https://smith.langchain.com/) - LLM オブザーバビリティ
